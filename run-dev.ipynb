{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from reco_encoder.data import input_layer\n",
    "from reco_encoder.model import model\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "import time\n",
    "from pathlib import Path\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_eval(encoder, evaluation_data_layer):\n",
    "    encoder.eval()\n",
    "    denom = 0.0\n",
    "    total_epoch_loss = 0.0\n",
    "    for i, (eval, src) in enumerate(evaluation_data_layer.iterate_one_epoch_eval()):\n",
    "        inputs = Variable(src.to_dense())\n",
    "        targets = Variable(eval.to_dense())\n",
    "        outputs = encoder(inputs)\n",
    "        loss, num_ratings = model.MSEloss(outputs, targets)\n",
    "        total_epoch_loss += loss.data[0]\n",
    "        denom += num_ratings.data[0]\n",
    "    return sqrt(total_epoch_loss / denom)\n",
    "\n",
    "\n",
    "def run(args_drop_prob, args_hidden_layers, \n",
    "        args_path_to_train_data, args_path_to_valid_data, args_path_to_test_data, \n",
    "        args_non_linearity_type, args_num_epochs, args_batch_size,\n",
    "        args_log_dir,\n",
    "        params): \n",
    "        \n",
    "    args_lr = 0.005 \n",
    "    args_weight_decay = 0\n",
    "    args_optimizer = \"momentum\"\n",
    "    args_aug_step =1\n",
    "    args_noise_prob = 0\n",
    "    args_constrained = True\n",
    "    args_skip_last_layer_nl = False\n",
    "\n",
    "    print(\"Loading training data\")\n",
    "\n",
    "    data_layer = input_layer.UserItemRecDataProvider(params=params)\n",
    "    print(\"Data loaded\")\n",
    "    print(\"Total items found: {}\".format(len(data_layer.data.keys())))\n",
    "    print(\"Vector dim: {}\".format(data_layer.vector_dim))\n",
    "\n",
    "    print(\"Loading valid data\")\n",
    "    eval_params = copy.deepcopy(params)\n",
    "    # eval_params['batch_size'] = 1\n",
    "    eval_params['data_dir'] = args_path_to_valid_data\n",
    "    eval_data_layer = input_layer.UserItemRecDataProvider(params=eval_params,\n",
    "                                                        user_id_map=data_layer.userIdMap, # the mappings are provided\n",
    "                                                        item_id_map=data_layer.itemIdMap)\n",
    "    eval_data_layer.src_data = data_layer.data\n",
    "    #'''\n",
    "    rencoder = model.AutoEncoder(layer_sizes=[data_layer.vector_dim] + args_hidden_layers,\n",
    "                               nl_type=args_non_linearity_type,\n",
    "                               is_constrained=args_constrained,\n",
    "                               dp_drop_prob=args_drop_prob,\n",
    "                               last_layer_activations=not args_skip_last_layer_nl)\n",
    "    '''\n",
    "    rencoder = model.AutoEncoder(hidden_layers=[data_layer.vector_dim] + args_hidden_layers,\n",
    "                               activation_type=args_non_linearity_type,\n",
    "                               is_constrained=args_constrained,\n",
    "                               drop_out_prob=args_drop_prob,\n",
    "                               activation_last_layer=not args_skip_last_layer_nl)\n",
    "    '''\n",
    "\n",
    "    model_checkpoint = args_log_dir + \"/model\"\n",
    "    path_to_model = Path(model_checkpoint)\n",
    "    if path_to_model.is_file():\n",
    "        print(\"Loading model from: {}\".format(model_checkpoint))\n",
    "        rencoder.load_state_dict(torch.load(model_checkpoint))\n",
    "\n",
    "    print('######################################################')\n",
    "    print('######################################################')\n",
    "    print('############# AutoEncoder Model: #####################')\n",
    "    print(rencoder)\n",
    "    print('######################################################')\n",
    "    print('######################################################')\n",
    "\n",
    "    if args_optimizer == \"adam\":\n",
    "        optimizer = optim.Adam(rencoder.parameters(),\n",
    "                           lr=args_lr,\n",
    "                           weight_decay=args_weight_decay)\n",
    "    elif args_optimizer == \"adagrad\":\n",
    "        optimizer = optim.Adagrad(rencoder.parameters(),\n",
    "                              lr=args_lr,\n",
    "                              weight_decay=args_weight_decay)\n",
    "    elif args_optimizer == \"momentum\":\n",
    "        optimizer = optim.SGD(rencoder.parameters(),\n",
    "                          lr=args_lr, momentum=0.9,\n",
    "                          weight_decay=args_weight_decay)\n",
    "        scheduler = MultiStepLR(optimizer, milestones=[24, 36, 48, 66, 72], gamma=0.5)\n",
    "    elif args_optimizer == \"rmsprop\":\n",
    "        optimizer = optim.RMSprop(rencoder.parameters(),\n",
    "                              lr=args_lr, momentum=0.9,\n",
    "                              weight_decay=args_weight_decay)\n",
    "\n",
    "    t_loss = 0.0\n",
    "    t_loss_denom = 0.0\n",
    "    global_step = 0 \n",
    "    summary_frequency = 100\n",
    "    \n",
    "    if args_noise_prob > 0.0:\n",
    "        dp = nn.Dropout(p=args_noise_prob)\n",
    "\n",
    "    for epoch in range(args_num_epochs):\n",
    "        print('Doing epoch {} of {}'.format(epoch, args_num_epochs))\n",
    "        e_start_time = time.time()\n",
    "        rencoder.train()\n",
    "        total_epoch_loss = 0.0\n",
    "        denom = 0.0\n",
    "        if args_optimizer == \"momentum\":\n",
    "            scheduler.step()\n",
    "        for i, mb in enumerate(data_layer.iterate_one_epoch()):\n",
    "            inputs = Variable(mb.to_dense())\n",
    "            optimizer.zero_grad()\n",
    "            outputs = rencoder(inputs)\n",
    "            loss, num_ratings = model.MSEloss(outputs, inputs)\n",
    "            loss = loss / num_ratings\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "            t_loss += loss.data[0]\n",
    "            t_loss_denom += 1\n",
    "    \n",
    "            if i % summary_frequency == 0:\n",
    "                print('[lei] t_loss: %.7f, t_loss_denom: %.7f' % (t_loss, t_loss_denom))\n",
    "                print('[%d, %5d] RMSE: %.7f' % (epoch, i, sqrt(1.0 * t_loss / t_loss_denom)))\n",
    "                t_loss = 0\n",
    "                t_loss_denom = 0.0\n",
    "\n",
    "            total_epoch_loss += loss.data[0]\n",
    "            denom += 1\n",
    "\n",
    "            #if args_aug_step > 0 and i % args_aug_step == 0 and i > 0:\n",
    "            if args_aug_step > 0:\n",
    "                # Magic data augmentation trick happen here\n",
    "                for t in range(args_aug_step):\n",
    "                    inputs = Variable(outputs.data)\n",
    "                    if args_noise_prob > 0.0:\n",
    "                        inputs = dp(inputs)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = rencoder(inputs)\n",
    "                    loss, num_ratings = model.MSEloss(outputs, inputs)\n",
    "                    loss = loss / num_ratings\n",
    "                    loss.backward()\n",
    "                    optimizer.step()    \n",
    "\n",
    "        e_end_time = time.time()\n",
    "        print('Total epoch {} finished in {} seconds with TRAINING RMSE loss: {}'\n",
    "              .format(epoch, e_end_time - e_start_time, sqrt(1.0 * total_epoch_loss/denom)))\n",
    "        if epoch % 3 == 0 or epoch == args_num_epochs - 1:\n",
    "            eval_loss = do_eval(rencoder, eval_data_layer)\n",
    "            print('Epoch {} EVALUATION LOSS: {}'.format(epoch, eval_loss))\n",
    "            print(\"Saving model to {}\".format(model_checkpoint + \".epoch_\"+str(epoch)))\n",
    "            torch.save(rencoder.state_dict(), model_checkpoint + \".epoch_\"+str(epoch))\n",
    "\n",
    "    print(\"Saving model to {}\".format(model_checkpoint + \".last\"))\n",
    "    torch.save(rencoder.state_dict(), model_checkpoint + \".last\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def infer(args_drop_prob, args_hidden_layers, \n",
    "        args_path_to_train_data, args_path_to_valid_data, args_path_to_test_data, \n",
    "        args_non_linearity_type, args_num_epochs, args_batch_size,\n",
    "        args_log_dir,\n",
    "        params):\n",
    "  \n",
    "    args_predictions_path = args_log_dir + \"/preds.txt\"\n",
    "    print(\"Loading training data\")\n",
    "    data_layer = input_layer.UserItemRecDataProvider(params=params)\n",
    "    print(\"Data loaded\")\n",
    "    print(\"Total items found: {}\".format(len(data_layer.data.keys())))\n",
    "    print(\"Vector dim: {}\".format(data_layer.vector_dim))\n",
    "\n",
    "    print(\"Loading eval data\")\n",
    "    eval_params = copy.deepcopy(params)\n",
    "    # must set eval batch size to 1 to make sure no examples are missed\n",
    "    eval_params['batch_size'] = 1\n",
    "    eval_params['data_dir'] = args_path_to_test_data\n",
    "    eval_data_layer = input_layer.UserItemRecDataProvider(params=eval_params,\n",
    "                                                          user_id_map=data_layer.userIdMap,\n",
    "                                                          item_id_map=data_layer.itemIdMap)\n",
    "\n",
    "    rencoder = model.AutoEncoder(layer_sizes=[data_layer.vector_dim] + args_hidden_layers,\n",
    "                               nl_type=args_non_linearity_type,\n",
    "                               is_constrained=True,\n",
    "                               dp_drop_prob=args_drop_prob,\n",
    "                               last_layer_activations=True)\n",
    "\n",
    "    path_to_model = Path(args_log_dir)\n",
    "    #print(\"Path to the model:{}\".format(path_to_model))\n",
    "    #if path_to_model.is_file():\n",
    "        #print(\"Loading model from: {}\".format(path_to_model))\n",
    "        #rencoder.load_state_dict(torch.load(path_to_model))\n",
    "    print(\"Loading model from: {}\".format(path_to_model))\n",
    "    rencoder.load_state_dict(torch.load(args_log_dir+'/model.last'))\n",
    "\n",
    "    print('######################################################')\n",
    "    print('######################################################')\n",
    "    print('############# AutoEncoder Model: #####################')\n",
    "    print(rencoder)\n",
    "    print('######################################################')\n",
    "    print('######################################################')\n",
    "    rencoder.eval()\n",
    "    inv_userIdMap = {v: k for k, v in data_layer.userIdMap.items()}\n",
    "    inv_itemIdMap = {v: k for k, v in data_layer.itemIdMap.items()}\n",
    "\n",
    "    eval_data_layer.src_data = data_layer.data\n",
    "    with open(args_predictions_path, 'w') as outf:\n",
    "        for i, ((out, src), majorInd) in enumerate(eval_data_layer.iterate_one_epoch_eval(for_inf=True)):\n",
    "            inputs = Variable(src.to_dense())\n",
    "            targets_np = out.to_dense().numpy()[0, :]\n",
    "            outputs = rencoder(inputs).cpu().data.numpy()[0, :]\n",
    "            non_zeros = targets_np.nonzero()[0].tolist()\n",
    "            major_key = inv_userIdMap[majorInd]\n",
    "            for ind in non_zeros:\n",
    "                outf.write(\"{}\\t{}\\t{}\\t{}\\n\".format(major_key, inv_itemIdMap[ind], outputs[ind], targets_np[ind]))\n",
    "            if i % 10000 == 0:\n",
    "                print(\"Done: {}\".format(i))\n",
    "    print(\"Total done: {}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "now_hash = str(datetime.now()).replace(' ', '|')\n",
    "args_drop_prob = 0.8\n",
    "args_hidden_layers = [512, 512, 1024]\n",
    "args_path_to_train_data = \"Netflix/SUB_TRAIN\"\n",
    "args_path_to_valid_data = \"Netflix/SUB_VALID\"\n",
    "args_path_to_test_data = \"Netflix/SUB_TEST\"\n",
    "args_non_linearity_type = \"selu\"\n",
    "args_num_epochs = 12\n",
    "args_batch_size = 128\n",
    "args_log_dir = \"model_save_{}\".format(now_hash)\n",
    "\n",
    "params = {\n",
    "    'batch_size': args_batch_size,\n",
    "    'data_dir': args_path_to_train_data,\n",
    "    'major': 'users',\n",
    "    'itemIdInd': 1,\n",
    "    'userIdInd': 0,\n",
    "    # 'delimiter': \",\"\n",
    "}\n",
    "\n",
    "if not os.path.exists(args_log_dir):\n",
    "    os.makedirs(args_log_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total epoch 0 finished in 56.01937174797058 seconds with TRAINING RMSE loss: 0.689641756732826\n",
      "Epoch 0 EVALUATION LOSS: 3.8512351986654445\n",
      "Saving model to model_save_2017-12-13|18:40:02.972474/model.epoch_0\n",
      "Doing epoch 1 of 12\n",
      "[lei] t_loss: 45.5169214, t_loss_denom: 96.0000000\n",
      "[1,     0] RMSE: 0.6885743\n",
      "Total epoch 1 finished in 55.82537508010864 seconds with TRAINING RMSE loss: 0.5675758032500774\n",
      "Doing epoch 2 of 12\n",
      "[lei] t_loss: 30.8367117, t_loss_denom: 96.0000000\n",
      "[2,     0] RMSE: 0.5667590\n",
      "Total epoch 2 finished in 54.199846029281616 seconds with TRAINING RMSE loss: 0.5228552466730471\n",
      "Doing epoch 3 of 12\n",
      "[lei] t_loss: 26.2594767, t_loss_denom: 96.0000000\n",
      "[3,     0] RMSE: 0.5230069\n",
      "Total epoch 3 finished in 55.48429584503174 seconds with TRAINING RMSE loss: 0.5070600960274384\n",
      "Epoch 3 EVALUATION LOSS: 3.8910174544333556\n",
      "Saving model to model_save_2017-12-13|18:40:02.972474/model.epoch_3\n",
      "Doing epoch 4 of 12\n",
      "[lei] t_loss: 24.7068621, t_loss_denom: 96.0000000\n",
      "[4,     0] RMSE: 0.5073097\n",
      "Total epoch 4 finished in 55.21540141105652 seconds with TRAINING RMSE loss: 0.4912856579507649\n",
      "Doing epoch 5 of 12\n",
      "[lei] t_loss: 23.1103855, t_loss_denom: 96.0000000\n",
      "[5,     0] RMSE: 0.4906457\n",
      "Total epoch 5 finished in 54.722994804382324 seconds with TRAINING RMSE loss: 0.48425976458566816\n",
      "Doing epoch 6 of 12\n",
      "[lei] t_loss: 22.5527747, t_loss_denom: 96.0000000\n",
      "[6,     0] RMSE: 0.4846904\n",
      "Total epoch 6 finished in 55.15476036071777 seconds with TRAINING RMSE loss: 0.46847453003508766\n",
      "Epoch 6 EVALUATION LOSS: 3.9008020798372005\n",
      "Saving model to model_save_2017-12-13|18:40:02.972474/model.epoch_6\n",
      "Doing epoch 7 of 12\n",
      "[lei] t_loss: 21.1058272, t_loss_denom: 96.0000000\n",
      "[7,     0] RMSE: 0.4688842\n",
      "Total epoch 7 finished in 55.818352460861206 seconds with TRAINING RMSE loss: 0.46280119765795946\n",
      "Doing epoch 8 of 12\n",
      "[lei] t_loss: 20.4358976, t_loss_denom: 96.0000000\n",
      "[8,     0] RMSE: 0.4613826\n",
      "Total epoch 8 finished in 55.359943151474 seconds with TRAINING RMSE loss: 0.45105301146513704\n",
      "Doing epoch 9 of 12\n",
      "[lei] t_loss: 19.5525308, t_loss_denom: 96.0000000\n",
      "[9,     0] RMSE: 0.4513006\n",
      "Total epoch 9 finished in 55.05738067626953 seconds with TRAINING RMSE loss: 0.43925538629891114\n",
      "Epoch 9 EVALUATION LOSS: 3.8769276757425715\n",
      "Saving model to model_save_2017-12-13|18:40:02.972474/model.epoch_9\n",
      "Doing epoch 10 of 12\n",
      "[lei] t_loss: 18.5427649, t_loss_denom: 96.0000000\n",
      "[10,     0] RMSE: 0.4394927\n",
      "Total epoch 10 finished in 53.78775119781494 seconds with TRAINING RMSE loss: 0.4327225070754888\n",
      "Doing epoch 11 of 12\n",
      "[lei] t_loss: 17.9694573, t_loss_denom: 96.0000000\n",
      "[11,     0] RMSE: 0.4326452\n",
      "Total epoch 11 finished in 55.15404009819031 seconds with TRAINING RMSE loss: 0.42442358596276525\n",
      "Epoch 11 EVALUATION LOSS: 3.8589754339600977\n",
      "Saving model to model_save_2017-12-13|18:40:02.972474/model.epoch_11\n",
      "Saving model to model_save_2017-12-13|18:40:02.972474/model.last\n"
     ]
    }
   ],
   "source": [
    "run(\n",
    "    args_drop_prob, args_hidden_layers, \n",
    "    args_path_to_train_data, args_path_to_valid_data, args_path_to_test_data, \n",
    "    args_non_linearity_type, args_num_epochs, args_batch_size,\n",
    "    args_log_dir,\n",
    "    params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infer(\n",
    "    args_drop_prob, args_hidden_layers, \n",
    "    args_path_to_train_data, args_path_to_valid_data, args_path_to_test_data, \n",
    "    args_non_linearity_type, args_num_epochs, args_batch_size,\n",
    "    args_log_dir,\n",
    "    params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file model_save_2017-12-12|04:27:28.508141/preds.txt\n",
      "####################\n",
      "RMSE: 3.8976483763475067\n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "def getRMSE(file):\n",
    "    print(\"file\", file)\n",
    "    n = 0\n",
    "    denom = 0.0\n",
    "    for line in open(file, 'r').readlines():\n",
    "        parts = line.split('\\t')\n",
    "        denom += (float(parts[2]) - float(parts[3])) ** 2\n",
    "        n += 1\n",
    "    print(\"####################\")\n",
    "    print(\"RMSE: {}\".format(sqrt(denom/n)))\n",
    "    print(\"####################\")\n",
    "    \n",
    "\n",
    "file = args_log_dir + \"/preds.txt\"\n",
    "getRMSE(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "args_log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_to_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
